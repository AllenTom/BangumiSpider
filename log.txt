2017-06-02 12:23:19 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: bangumi)
2017-06-02 12:23:19 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'bangumi', 'LOG_FILE': 'log.txt', 'NEWSPIDER_MODULE': 'bangumi.spiders', 'RETRY_HTTP_CODES': [500, 503, 504, 400, 403, 404, 408], 'RETRY_TIMES': 3, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['bangumi.spiders']}
2017-06-02 12:23:19 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-06-02 12:23:19 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'bangumi.RandomProxy',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'bangumi.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-06-02 12:23:19 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-06-02 12:23:19 [scrapy.middleware] INFO: Enabled item pipelines:
['bangumi.pipelines.ImageDownloadPipeline',
 'bangumi.pipelines.BangumiIDPipelines',
 'bangumi.pipelines.BangumiGamePipelines',
 'bangumi.pipelines.BangumiCharacterPipelines',
 'bangumi.pipelines.BangumiAnimatePipelines',
 'bangumi.pipelines.BangumiBookPipelines',
 'bangumi.pipelines.BangumiPersonPipelines',
 'bangumi.pipelines.BangumiMusicPipelines']
2017-06-02 12:23:19 [scrapy.core.engine] INFO: Spider opened
2017-06-02 12:23:19 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-06-02 12:23:19 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-06-02 12:23:19 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:23:19 [scrapy.proxies] DEBUG: Using proxy <http://221.234.36.48:8080>, 100 proxies left
2017-06-02 12:23:20 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/robots.txt> (referer: None)
2017-06-02 12:23:20 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:23:20 [scrapy.proxies] DEBUG: Using proxy <http://121.207.40.201:40896>, 100 proxies left
2017-06-02 12:23:21 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/subject/9912> (referer: None)
2017-06-02 12:23:21 [bangumi_animate] INFO: Parse function called on http://bangumi.tv/subject/9912
2017-06-02 12:23:21 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:23:21 [scrapy.proxies] DEBUG: Using proxy <http://111.76.224.59:808>, 100 proxies left
2017-06-02 12:23:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/subject/9912/ep> (referer: http://bangumi.tv/subject/9912)
2017-06-02 12:23:24 [scrapy.core.engine] INFO: Closing spider (finished)
2017-06-02 12:23:24 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1006,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 22150,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 6, 2, 4, 23, 24, 7297),
 'log_count/DEBUG': 10,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 6, 2, 4, 23, 19, 963860)}
2017-06-02 12:23:24 [scrapy.core.engine] INFO: Spider closed (finished)
2017-06-02 12:23:41 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: bangumi)
2017-06-02 12:23:41 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'bangumi', 'LOG_FILE': 'log.txt', 'NEWSPIDER_MODULE': 'bangumi.spiders', 'RETRY_HTTP_CODES': [500, 503, 504, 400, 403, 404, 408], 'RETRY_TIMES': 3, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['bangumi.spiders']}
2017-06-02 12:23:41 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-06-02 12:23:41 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'bangumi.RandomProxy',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'bangumi.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-06-02 12:23:41 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-06-02 12:23:41 [scrapy.middleware] INFO: Enabled item pipelines:
['bangumi.pipelines.ImageDownloadPipeline',
 'bangumi.pipelines.BangumiIDPipelines',
 'bangumi.pipelines.BangumiGamePipelines',
 'bangumi.pipelines.BangumiCharacterPipelines',
 'bangumi.pipelines.BangumiAnimatePipelines',
 'bangumi.pipelines.BangumiBookPipelines',
 'bangumi.pipelines.BangumiPersonPipelines',
 'bangumi.pipelines.BangumiMusicPipelines']
2017-06-02 12:23:41 [scrapy.core.engine] INFO: Spider opened
2017-06-02 12:23:41 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-06-02 12:23:41 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-06-02 12:23:41 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:23:41 [scrapy.proxies] DEBUG: Using proxy <http://219.144.45.18:8998>, 100 proxies left
2017-06-02 12:23:47 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/robots.txt> (referer: None)
2017-06-02 12:23:47 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:23:47 [scrapy.proxies] DEBUG: Using proxy <http://221.5.5.232:8081>, 100 proxies left
2017-06-02 12:23:49 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/subject/9912> (referer: None)
2017-06-02 12:23:49 [bangumi_animate] INFO: Parse function called on http://bangumi.tv/subject/9912
2017-06-02 12:23:50 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:23:50 [scrapy.proxies] DEBUG: Using proxy <http://117.65.5.21:8998>, 100 proxies left
2017-06-02 12:23:51 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/subject/9912/ep> (referer: http://bangumi.tv/subject/9912)
2017-06-02 12:23:51 [scrapy.core.engine] INFO: Closing spider (finished)
2017-06-02 12:23:51 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 1099,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 22161,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 6, 2, 4, 23, 51, 163772),
 'log_count/DEBUG': 10,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 6, 2, 4, 23, 41, 913460)}
2017-06-02 12:23:51 [scrapy.core.engine] INFO: Spider closed (finished)
2017-06-02 12:25:30 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: bangumi)
2017-06-02 12:25:30 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'bangumi', 'LOG_FILE': 'log.txt', 'NEWSPIDER_MODULE': 'bangumi.spiders', 'RETRY_HTTP_CODES': [500, 503, 504, 400, 403, 404, 408], 'RETRY_TIMES': 3, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['bangumi.spiders']}
2017-06-02 12:25:30 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2017-06-02 12:25:30 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
 'bangumi.RandomProxy',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'bangumi.middlewares.RandomUserAgentMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2017-06-02 12:25:30 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2017-06-02 12:25:30 [scrapy.middleware] INFO: Enabled item pipelines:
['bangumi.pipelines.ImageDownloadPipeline',
 'bangumi.pipelines.BangumiIDPipelines',
 'bangumi.pipelines.BangumiGamePipelines',
 'bangumi.pipelines.BangumiCharacterPipelines',
 'bangumi.pipelines.BangumiAnimatePipelines',
 'bangumi.pipelines.BangumiBookPipelines',
 'bangumi.pipelines.BangumiPersonPipelines',
 'bangumi.pipelines.BangumiMusicPipelines']
2017-06-02 12:25:30 [scrapy.core.engine] INFO: Spider opened
2017-06-02 12:25:30 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2017-06-02 12:25:30 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2017-06-02 12:25:30 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:25:30 [scrapy.proxies] DEBUG: Using proxy <http://123.138.43.92:8080>, 100 proxies left
2017-06-02 12:25:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/robots.txt> (referer: None)
2017-06-02 12:25:31 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:25:31 [scrapy.proxies] DEBUG: Using proxy <http://60.211.209.79:8080>, 100 proxies left
2017-06-02 12:25:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/subject/9912> (referer: None)
2017-06-02 12:25:33 [bangumi_animate] INFO: Parse function called on http://bangumi.tv/subject/9912
2017-06-02 12:25:33 [scrapy.proxies] DEBUG: Proxy user pass not found
2017-06-02 12:25:33 [scrapy.proxies] DEBUG: Using proxy <http://124.112.78.239:8998>, 100 proxies left
2017-06-02 12:25:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://bangumi.tv/subject/9912/ep> (referer: http://bangumi.tv/subject/9912)
2017-06-02 12:25:36 [scrapy.core.engine] INFO: Closing spider (finished)
2017-06-02 12:25:36 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 934,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 22160,
 'downloader/response_count': 3,
 'downloader/response_status_count/200': 3,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2017, 6, 2, 4, 25, 36, 779426),
 'log_count/DEBUG': 10,
 'log_count/INFO': 8,
 'request_depth_max': 1,
 'response_received_count': 3,
 'scheduler/dequeued': 2,
 'scheduler/dequeued/memory': 2,
 'scheduler/enqueued': 2,
 'scheduler/enqueued/memory': 2,
 'start_time': datetime.datetime(2017, 6, 2, 4, 25, 30, 667101)}
2017-06-02 12:25:36 [scrapy.core.engine] INFO: Spider closed (finished)
